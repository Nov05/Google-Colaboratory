{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AmqsRk0EwK5HCqCnq_IBSqADvtwzVIRL",
      "authorship_tag": "ABX9TyMQJvCIY1aC1fm23k/RmM4+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nov05/Google-Colaboratory/blob/master/20240102_solve_openAI_gym's_taxi_v2_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **\\<TOP>**\n",
        "\n",
        "* created by nov05 on 2020-05-28. changed on 2024-01-02  \n",
        "* project folder [on google drive](https://drive.google.com/drive/folders/1I_xoXvtDxSrTJ-CkFE662q_MZrtuiYch), [on github](https://github.com/udacity/deep-reinforcement-learning)    \n",
        "* go to the course [\"Solve OpenAI Gym's Taxi-v2 Task\"](https://learn.udacity.com/nanodegrees/nd893/parts/6f8342e1-2278-4998-a384-283c136c9f69/lessons/d122ea3e-79eb-457a-bdf1-6c37ff3f18c8/concepts/cc1028c3-f559-4d18-a09a-17f1fef2c6e8)    \n",
        "* go to [the \"Temporal Difference\" notebook](https://drive.google.com/file/d/1itgKIuuoXybSGnWwO1PfUsQPAk0ZmPKh)  "
      ],
      "metadata": {
        "id": "X57ADQIcQfYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **OpenAI Gym's Taxi-v2 Task**    \n",
        "\n",
        "Before proceeding, read the description of the environment in subsection 3.1 of this paper.  \n",
        "https://arxiv.org/pdf/cs/9905014.pdf\n",
        "\n",
        "```\n",
        "To make the discussion concrete, let us consider the following simple example. Figure 1 shows\n",
        "a 5-by-5 grid world inhabited by a taxi agent. There are four specially-designated locations in\n",
        "this world, marked as R(ed), B(lue), G(reen), and Y(ellow). The taxi problem is episodic. In\n",
        "each episode, the taxi starts in a randomly-chosen square. There is a passenger at one of the\n",
        "four locations (chosen randomly), and that passenger wishes to be transported to one of the four\n",
        "locations (also chosen randomly). The taxi must go to the passenger’s location (the “source”), pick\n",
        "up the passenger, go to the destination location (the “destination”), and put down the passenger\n",
        "there. (To keep things uniform, the taxi must pick up and drop off the passenger even if he/she\n",
        "is already located at the destination!) The episode ends when the passenger is deposited at the\n",
        "destination location.\n",
        "\n",
        "There are six primitive actions in this domain:\n",
        "(a) four navigation actions that move the taxi\n",
        "one square North, South, East, or West,\n",
        "(b) a Pickup action, and\n",
        "(c) a Putdown action. Each action\n",
        "is deterministic. There is a reward of −1 for each action and an additional reward of +20 for\n",
        "successfully delivering the passenger. There is a reward of −10 if the taxi attempts to execute the\n",
        "Putdown or Pickup actions illegally. If a navigation action would cause the taxi to hit a wall, the\n",
        "action is a no-op, and there is only the usual reward of −1.\n",
        "\n",
        "We seek a policy that maximizes the total reward per episode. There are 500 possible states:\n",
        "25 squares, 5 locations for the passenger (counting the four starting locations and the taxi), and 4\n",
        "destinations.\n",
        "```\n",
        "\n",
        "<img src=\"https://github.com/Nov05/pictures/blob/master/Udacity/20231221_reinforcement%20learning/taxi-v2.png?raw=true\" width=200>"
      ],
      "metadata": {
        "id": "jZ_naFmBaDQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Workspace**  \n",
        "  \n",
        "The workspace contains three files:\n",
        "\n",
        "* **agent.py**: Develop your reinforcement learning agent here. This is the only file that you should modify.\n",
        "* **monitor.py**: The **interact** function tests how well your agent learns from interaction with the environment.\n",
        "* **main.py**: Run this file in the terminal to check the performance of your agent.\n",
        "\n",
        "When you run **main.py**, the agent that you specify in agent.py interacts with the environment for 20,000 episodes. The details of the interaction are specified in monitor.py, which returns two variables: **avg_rewards** and **best_avg_reward**.\n",
        "\n",
        "* **avg_rewards** is a deque where **avg_rewards[i]** is the average (undiscounted) return collected by the agent from episodes **i+1** to episode **i+100**, inclusive. So, for instance, **avg_rewards[0]** is the average return collected by the agent over the first 100 episodes.\n",
        "* **best_avg_reward** is the largest entry in **avg_rewards**. This is the final score that you should use when determining how well your agent performed in the task.\n",
        "\n",
        "Your assignment is to modify the agents.py file to improve the agent's performance.\n",
        "\n",
        "* Use the **__init__()** method to define any needed instance variables. Currently, we define the number of actions available to the agent **(nA)** and initialize the action values (**Q**) to an empty dictionary of arrays. Feel free to add more instance variables; for example, you may find it useful to define the value of epsilon if the agent uses an epsilon-greedy policy for selecting actions.\n",
        "* The **select_action()** method accepts the environment state as input and returns the agent's choice of action. The default code that we have provided randomly selects an action.\n",
        "* The **step()** method accepts a (**state**, **action**, **reward**, **next_state**) tuple as input, along with the **done** variable, which is **True** if the episode has ended. The default code (which you should certainly change!) increments the action value of the previous state-action pair by 1. You should change this method to use the sampled tuple of experience to update the agent's knowledge of the problem."
      ],
      "metadata": {
        "id": "xVs1RDQE1hPE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2bb_0nHw3QC"
      },
      "source": [
        "# **Evaluate your Performance**\n",
        "\n",
        "OpenAI Gym defines [\"solving\"](https://www.gymlibrary.dev/environments/toy_text/taxi/) this task as getting **average return of 9.7** over 100 consecutive trials.\n",
        "\n",
        "While this coding exercise is ungraded, we recommend that you try to attain an average return of at least 9.1 over 100 consecutive trials (**best_avg_reward > 9.1**).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Not sure where to start?**\n",
        "\n",
        "Note that this exercise is intentionally open-ended, and we won't provide an official solution. For help with this exercise, please reach out to your instructors and fellow students! As a first step, you should figure out how to adapt your implementation in the **Temporal-Difference Methods** lesson to implement an agent to learn in this new environment. The code will likely be very similar to the notebook from the Temporal-Difference Methods lesson, where you need only modify very few things to fit this slightly different format."
      ],
      "metadata": {
        "id": "SfN6yXvq1ums"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Share your Results**  \n",
        "\n",
        "If you arrive at an implementation that you are proud of, please share your results with the student community! You can also reach out to ask questions, get implementation hints, share ideas, or find collaborators!\n",
        "\n",
        "As a final step, towards sharing your ideas with the wider RL community, you may like to create a write-up and submit it to the OpenAI Gym Leaderboard! https://github.com/openai/gym/wiki/Leaderboard"
      ],
      "metadata": {
        "id": "zoJiQXPl1yqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Code**  "
      ],
      "metadata": {
        "id": "FxzBHMe014p4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MxSwd4twhlE"
      },
      "source": [
        "## get the \"lab-taxi\" folder\n",
        "!gdown --no-check-certificate --folder https://drive.google.com/drive/folders/10uOw8cEHoE2gDn9ZTAi3EuIQWGiQuq6e\n",
        "!pip uninstall -y gym ## colab pre-installed 0.25.2\n",
        "!pip install gym==0.9.6\n",
        "import gym\n",
        "print(gym.__version__)\n",
        "## restart the session \"Ctrl+M.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd lab-taxi\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik8tPOCfKojr",
        "outputId": "5da5d8f3-d836-40d0-a410-1d454303ac80"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lab-taxi\n",
            "/content/lab-taxi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make('Taxi-v2')\n",
        "env.action_space.n ## 6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_W_GqTu6lOs",
        "outputId": "efe3e4b0-fb06-42c7-c373-bda28d01e55b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPI6m9nkxW3A"
      },
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "class Agent:\n",
        "\n",
        "\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\" Initialize agent.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "        self.alpha = .01\n",
        "        self.gamma = 1.0\n",
        "        self.epsilon = 1./20000 ## for Ɛ-greedy(Q)\n",
        "\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\" Given the state, select an action.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - state: the current state of the environment\n",
        "\n",
        "        Returns\n",
        "        =======\n",
        "        - action: an integer, compatible with the task's action space\n",
        "        \"\"\"\n",
        "        # return np.random.choice(self.nA) ## equiprobable random policy\n",
        "        if state in self.Q: ## Ɛ-greedy(Q)\n",
        "            policy_s = self.get_policy_for_state(self.Q[state])\n",
        "            return np.random.choice(np.arange(self.nA), p=policy_s)\n",
        "        else: ## equiprobable random policy\n",
        "            return np.random.choice(self.nA)\n",
        "\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\" Update the agent's knowledge, using the most recently sampled tuple.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - state: the previous state of the environment\n",
        "        - action: the agent's previous choice of action\n",
        "        - reward: last reward received\n",
        "        - next_state: the current state of the environment\n",
        "        - done: whether the episode is complete (True or False)\n",
        "        \"\"\"\n",
        "        # self.Q[state][action] += 1 ## equiprobable random policy\n",
        "        if done:\n",
        "            self.update_Q_sarsamax(state, action, reward)\n",
        "            return\n",
        "        next_action = self.select_action(next_state)\n",
        "        self.update_Q_sarsamax(state, action, reward, next_state)\n",
        "        state, action = next_state, next_action\n",
        "\n",
        "\n",
        "    def get_policy_for_state(self, Q_s): ## Ɛ-greedy(Q)\n",
        "        policy_s = self.epsilon/self.nA * np.ones(self.nA)\n",
        "        policy_s[np.argmax(Q_s)] = 1 - self.epsilon + self.epsilon/self.nA\n",
        "        return policy_s\n",
        "\n",
        "\n",
        "    def update_Q_sarsamax(self, state, action, reward, next_state=None):\n",
        "        ## CAUTION: next_state could be 0\n",
        "        Q_sa_next = self.Q[next_state].max() if next_state is not None else 0\n",
        "        self.Q[state][action] = (1-self.alpha) * self.Q[state][action] + \\\n",
        "                                self.alpha * (reward + self.gamma*Q_sa_next)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQhOdRT1yori",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6222359c-93b0-46d1-c484-78d18d1e68ab"
      },
      "source": [
        "%%time\n",
        "# from agent import Agent\n",
        "from monitor import interact\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('Taxi-v2')\n",
        "agent = Agent()\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "## Wall time: 2min 46s\n",
        "## equiprobable random policy: Episode 20000/20000 || Best average reward -723.32\n",
        "## Q-Learning (alpha=.01, epsilon=1/20000): Episode 20000/20000 || Best average reward 9.05, 9.29\n",
        "##                        epsilon=1/10000, 9.2, (1/5000, 9.0), (1/15000, 9.07), (1/21000, 9.01), (1/30000, 9.0)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 20000/20000 || Best average reward 9.21\n",
            "\n",
            "CPU times: user 2min 29s, sys: 4.58 s, total: 2min 34s\n",
            "Wall time: 2min 43s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **\\<BOTTOM>**"
      ],
      "metadata": {
        "id": "BYNDR1017U7I"
      }
    }
  ]
}